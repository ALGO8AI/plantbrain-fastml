{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bab265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results (CV + Test):\n",
      "                   cv_rmse_mean  cv_rmse_std  cv_mae_mean  cv_mae_std  \\\n",
      "model                                                                   \n",
      "linear_regression      0.804085     0.130024     0.547236    0.003277   \n",
      "elastic_net            0.879462     0.008786     0.682731    0.005334   \n",
      "knn_regression         0.633650     0.010226     0.437635    0.004728   \n",
      "bayesian_ridge         0.804091     0.130040     0.547226    0.003273   \n",
      "decision_tree          0.714191     0.016612     0.452715    0.008101   \n",
      "lasso                  0.977362     0.010784     0.770404    0.007469   \n",
      "ridge                  0.804086     0.130027     0.547234    0.003276   \n",
      "adaboost               0.830984     0.078340     0.696649    0.084215   \n",
      "gradient_boosting      0.535417     0.013299     0.370083    0.005052   \n",
      "random_forest          0.514039     0.010519     0.332940    0.003713   \n",
      "svr                    0.767928     0.015141     0.540767    0.008894   \n",
      "\n",
      "                   cv_r2_mean  cv_r2_std  test_rmse  test_mae   test_r2  \n",
      "model                                                                    \n",
      "linear_regression    0.500343   0.181534   0.717256  0.529131  0.620670  \n",
      "elastic_net          0.419019   0.005075   0.871972  0.671467  0.439373  \n",
      "knn_regression       0.698314   0.009537   0.625272  0.433159  0.711726  \n",
      "bayesian_ridge       0.500333   0.181560   0.717252  0.529112  0.620674  \n",
      "decision_tree        0.616702   0.016109   0.686138  0.435018  0.652870  \n",
      "lasso                0.282509   0.002910   0.979344  0.764357  0.292804  \n",
      "ridge                0.500341   0.181539   0.717255  0.529127  0.620671  \n",
      "adaboost             0.478455   0.091410   0.762108  0.619810  0.571745  \n",
      "gradient_boosting    0.784548   0.010163   0.508634  0.353955  0.809244  \n",
      "random_forest        0.801458   0.007263   0.502257  0.326099  0.813997  \n",
      "svr                  0.556996   0.012839   0.735789  0.520067  0.600814  \n",
      "\n",
      "Best model by RMSE: random_forest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from plantbrain_fastml.managers.regressor_manager import RegressorManager\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import time\n",
    "# Load California housing dataset as DataFrame for compatibility\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Initialize manager and add models (if not already added)\n",
    "manager = RegressorManager()\n",
    "# (If RegressorManager already adds models in __init__, no need to add explicitly)\n",
    "# Otherwise, you can add explicitly:\n",
    "# manager.add_model(\"linear_regression\", LinearRegressionRegressor())\n",
    "# manager.add_model(\"random_forest\", RandomForestRegressorWrapper())\n",
    "\n",
    "# Evaluate all models on training data with hypertuning and feature elimination\n",
    "time_start = time.time()\n",
    "results = manager.evaluate_all(\n",
    "    X_train, y_train,\n",
    "    hypertune=False,\n",
    "    hypertune_params={'n_trials': 2},\n",
    "    n_jobs=6,  # Set to -1 for all cores\n",
    "    cv_folds=5,\n",
    "    test_size=0.1,\n",
    "    feature_elimination=True,\n",
    "    fe_n_features=5,\n",
    "    fe_method='lasso',\n",
    "    return_plots=True  # set True if you want plots\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results (CV + Test):\")\n",
    "print(results)\n",
    "\n",
    "# Get best model by metric (e.g., 'rmse')\n",
    "best_model_name, best_model = manager.get_best_model(metric='rmse', higher_is_better=False)\n",
    "print(f\"\\nBest model by RMSE: {best_model_name}\")\n",
    "time_end = time.time()\n",
    "\n",
    "# Evaluate best model on the test set separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for evaluation: 1.0 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken for evaluation: {(time_end - time_start)//60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92923970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear_regression': {},\n",
       " 'random_forest': {},\n",
       " 'decision_tree': {},\n",
       " 'svr': {},\n",
       " 'knn_regression': {},\n",
       " 'gradient_boosting': {},\n",
       " 'elastic_net': {},\n",
       " 'bayesian_ridge': {},\n",
       " 'adaboost': {},\n",
       " 'lasso': {},\n",
       " 'ridge': {}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ccb775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b7343f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlinear_regression\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscatter\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'scatter'"
     ]
    }
   ],
   "source": [
    "manager.get_plots()['linear_regression']['scatter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b099f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_scores': {'rmse': (np.float64(0.8040853634139525),\n",
       "   np.float64(0.1300242365136351)),\n",
       "  'mae': (np.float64(0.5472359711631587), np.float64(0.003277065357366334)),\n",
       "  'r2': (np.float64(0.5003432933141967), np.float64(0.1815338559505305))},\n",
       " 'test_scores': {'rmse': np.float64(0.717256211776781),\n",
       "  'mae': 0.5291309251606496,\n",
       "  'r2': 0.6206697989665664},\n",
       " 'plots': {'line': <Figure size 640x480 with 1 Axes>,\n",
       "  'scatter': <Figure size 640x480 with 1 Axes>}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Diabetes dataset...\n",
      "Dataset loaded successfully.\n",
      "Features shape: (442, 10)\n",
      "Target shape: (442,)\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set size: 353\n",
      "Test set size: 89\n",
      "\n",
      "Initializing the RegressorManager...\n",
      "RegressorManager initialized.\n",
      "Available models: ['linear_regression', 'random_forest', 'decision_tree', 'svr', 'knn_regression', 'gradient_boosting', 'elastic_net', 'bayesian_ridge', 'adaboost', 'lasso', 'ridge']\n",
      "\n",
      "Starting model evaluation...\n",
      "\n",
      "Evaluation completed in 2.56 seconds.\n",
      "\n",
      "--- Evaluation Results (Cross-Validation & Test Scores) ---\n",
      "                   cv_rmse_mean  cv_rmse_std  cv_mae_mean  cv_mae_std  \\\n",
      "model                                                                   \n",
      "linear_regression     56.607819     0.976782    45.406853    0.920921   \n",
      "random_forest         60.790774     2.901088    47.973298    2.717225   \n",
      "decision_tree         79.080974     3.550187    60.581560    2.079061   \n",
      "svr                   74.856356     1.409415    61.493548    0.649970   \n",
      "knn_regression        62.059391     1.702330    48.502837    1.594288   \n",
      "gradient_boosting     61.068662     0.973179    48.003238    1.505592   \n",
      "elastic_net           78.434602     1.480882    66.799008    0.769031   \n",
      "bayesian_ridge        56.599163     0.761051    45.420951    1.039664   \n",
      "adaboost              61.698492     2.485764    50.823115    2.533862   \n",
      "lasso                 63.055005     1.702777    53.300528    0.831282   \n",
      "ridge                 65.080525     1.644913    55.092751    0.236639   \n",
      "\n",
      "                   cv_r2_mean  cv_r2_std  test_rmse   test_mae   test_r2  \n",
      "model                                                                     \n",
      "linear_regression    0.478572   0.035343  54.532749  45.188425  0.446862  \n",
      "random_forest        0.399590   0.044263  60.208064  47.439718  0.325739  \n",
      "decision_tree       -0.018574   0.102996  84.719088  63.352113 -0.335000  \n",
      "svr                  0.090006   0.027464  72.837586  60.260652  0.013199  \n",
      "knn_regression       0.372543   0.057097  60.670061  50.816901  0.315352  \n",
      "gradient_boosting    0.394542   0.005655  59.931690  48.648929  0.331915  \n",
      "elastic_net          0.001378   0.002939  75.668560  62.671827 -0.065000  \n",
      "bayesian_ridge       0.478950   0.031150  54.396745  45.069592  0.449618  \n",
      "adaboost             0.382367   0.024615  56.385330  45.625598  0.408641  \n",
      "lasso                0.354783   0.009046  60.228450  51.365162  0.325283  \n",
      "ridge                0.312624   0.008245  58.931113  49.654780  0.354037  \n",
      "\n",
      "--- Getting the Best Model ---\n",
      "Best performing model based on RMSE: 'bayesian_ridge'\n",
      "\n",
      "--- Making Predictions with the Best Model ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- age\n- s2\n- s3\n- s4\n- s6\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# --- 5. Make Predictions with the Best Model ---\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Making Predictions with the Best Model ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m predictions = \u001b[43mbest_model_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPredictions made on the external test set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m sample_comparison = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mActual\u001b[39m\u001b[33m'\u001b[39m: y_test.values, \u001b[33m'\u001b[39m\u001b[33mPredicted\u001b[39m\u001b[33m'\u001b[39m: predictions}).head(\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\PLANTBRAIN\\plantbrain-fastml\\plantbrain_fastml\\models\\regressors\\bayesian_ridge.py:15\u001b[39m, in \u001b[36mBayesianRidge.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\12345\\Desktop\\PLANTBRAIN\\plantbrain-fastml\\brain\\Lib\\site-packages\\sklearn\\linear_model\\_bayes.py:398\u001b[39m, in \u001b[36mBayesianRidge.predict\u001b[39m\u001b[34m(self, X, return_std)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, return_std=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    377\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Predict using the linear model.\u001b[39;00m\n\u001b[32m    378\u001b[39m \n\u001b[32m    379\u001b[39m \u001b[33;03m    In addition to the mean of the predictive distribution, also its\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        Standard deviation of predictive distribution of query points.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     y_mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_std:\n\u001b[32m    400\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m y_mean\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\12345\\Desktop\\PLANTBRAIN\\plantbrain-fastml\\brain\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:277\u001b[39m, in \u001b[36mLinearModel._decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    275\u001b[39m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     coef_ = \u001b[38;5;28mself\u001b[39m.coef_\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coef_.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\12345\\Desktop\\PLANTBRAIN\\plantbrain-fastml\\brain\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\12345\\Desktop\\PLANTBRAIN\\plantbrain-fastml\\brain\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- age\n- s2\n- s3\n- s4\n- s6\n"
     ]
    }
   ],
   "source": [
    "from plantbrain_fastml.managers.regressor_manager import RegressorManager\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load and Prepare the Dataset ---\n",
    "print(\"Loading the Diabetes dataset...\")\n",
    "data = load_diabetes()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "print(\"\\nSplitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# --- 2. Initialize the Regressor Manager ---\n",
    "print(\"\\nInitializing the RegressorManager...\")\n",
    "manager = RegressorManager()\n",
    "print(\"RegressorManager initialized.\")\n",
    "print(\"Available models:\", list(manager.models.keys()))\n",
    "\n",
    "\n",
    "# --- 3. Evaluate All Models Using Default Metrics ---\n",
    "print(\"\\nStarting model evaluation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate all models WITHOUT passing a custom metrics list.\n",
    "# This will use the default metrics, which include 'rmse'.\n",
    "results = manager.evaluate_all(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    hypertune=False,\n",
    "    hypertune_params={'n_trials': 5},\n",
    "    n_jobs=1,\n",
    "    cv_folds=3,\n",
    "    test_size=0.2,\n",
    "    feature_elimination=True,\n",
    "    fe_n_features=5,\n",
    "    fe_method='lasso',\n",
    "    return_plots=True\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nEvaluation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- 4. Display Results and Get Best Model ---\n",
    "print(\"\\n--- Evaluation Results (Cross-Validation & Test Scores) ---\")\n",
    "print(results)\n",
    "\n",
    "print(\"\\n--- Getting the Best Model ---\")\n",
    "# Get the best model by 'rmse'. Lower is better.\n",
    "best_model_name, best_model_object = manager.get_best_model(metric='rmse', higher_is_better=False)\n",
    "print(f\"Best performing model based on RMSE: '{best_model_name}'\")\n",
    "\n",
    "\n",
    "# --- 5. Make Predictions with the Best Model ---\n",
    "print(\"\\n--- Making Predictions with the Best Model ---\")\n",
    "# best_model_object was returned from the get_best_model call earlier\n",
    "\n",
    "# Use the best model's own fitted preprocessor to transform the test data\n",
    "print(\"Applying the same feature elimination to the test set...\")\n",
    "X_test_processed = best_model_object.preprocessor.transform(X_test)\n",
    "\n",
    "# Now make predictions on the data that has the correct 5 features\n",
    "predictions = best_model_object.predict(X_test_processed)\n",
    "\n",
    "print(\"\\nPredictions made on the processed test set.\")\n",
    "# Display sample predictions vs. actual values\n",
    "sample_comparison = pd.DataFrame({'Actual': y_test.values, 'Predicted': predictions}).head(10)\n",
    "print(sample_comparison)\n",
    "\n",
    "# --- 6. Display Plots for the Best Model ---\n",
    "print(f\"\\n--- Displaying Plots for '{best_model_name}' ---\")\n",
    "plots = manager.get_plots()\n",
    "\n",
    "if plots and best_model_name in plots:\n",
    "    # Show the scatter plot (Predicted vs. Actual)\n",
    "    scatter_plot = plots[best_model_name].get('scatter')\n",
    "    if scatter_plot:\n",
    "        print(\"Displaying Predicted vs. Actual scatter plot...\")\n",
    "        scatter_plot.show()\n",
    "\n",
    "    # Show the feature importance plot (if available)\n",
    "    feature_importance_plot = plots[best_model_name].get('feature_importance')\n",
    "    if feature_importance_plot:\n",
    "        print(\"\\nDisplaying Feature Importance plot...\")\n",
    "        feature_importance_plot.show()\n",
    "else:\n",
    "    print(f\"No plots found for model '{best_model_name}'.\")\n",
    "\n",
    "print(\"\\nNotebook execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b25413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "total_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f800c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c2172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d50c2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_effective_n_jobs(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
