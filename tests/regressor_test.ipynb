{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bab265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results (CV + Test):\n",
      "                   cv_rmse_mean  cv_rmse_std  cv_mae_mean  cv_mae_std  \\\n",
      "model                                                                   \n",
      "linear_regression      0.804085     0.130024     0.547236    0.003277   \n",
      "elastic_net            0.879462     0.008786     0.682731    0.005334   \n",
      "knn_regression         0.633650     0.010226     0.437635    0.004728   \n",
      "bayesian_ridge         0.804091     0.130040     0.547226    0.003273   \n",
      "decision_tree          0.714191     0.016612     0.452715    0.008101   \n",
      "lasso                  0.977362     0.010784     0.770404    0.007469   \n",
      "ridge                  0.804086     0.130027     0.547234    0.003276   \n",
      "adaboost               0.830984     0.078340     0.696649    0.084215   \n",
      "gradient_boosting      0.535417     0.013299     0.370083    0.005052   \n",
      "random_forest          0.514039     0.010519     0.332940    0.003713   \n",
      "svr                    0.767928     0.015141     0.540767    0.008894   \n",
      "\n",
      "                   cv_r2_mean  cv_r2_std  test_rmse  test_mae   test_r2  \n",
      "model                                                                    \n",
      "linear_regression    0.500343   0.181534   0.717256  0.529131  0.620670  \n",
      "elastic_net          0.419019   0.005075   0.871972  0.671467  0.439373  \n",
      "knn_regression       0.698314   0.009537   0.625272  0.433159  0.711726  \n",
      "bayesian_ridge       0.500333   0.181560   0.717252  0.529112  0.620674  \n",
      "decision_tree        0.616702   0.016109   0.686138  0.435018  0.652870  \n",
      "lasso                0.282509   0.002910   0.979344  0.764357  0.292804  \n",
      "ridge                0.500341   0.181539   0.717255  0.529127  0.620671  \n",
      "adaboost             0.478455   0.091410   0.762108  0.619810  0.571745  \n",
      "gradient_boosting    0.784548   0.010163   0.508634  0.353955  0.809244  \n",
      "random_forest        0.801458   0.007263   0.502257  0.326099  0.813997  \n",
      "svr                  0.556996   0.012839   0.735789  0.520067  0.600814  \n",
      "\n",
      "Best model by RMSE: random_forest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from plantbrain_fastml.managers.regressor_manager import RegressorManager\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import time\n",
    "# Load California housing dataset as DataFrame for compatibility\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Initialize manager and add models (if not already added)\n",
    "manager = RegressorManager()\n",
    "# (If RegressorManager already adds models in __init__, no need to add explicitly)\n",
    "# Otherwise, you can add explicitly:\n",
    "# manager.add_model(\"linear_regression\", LinearRegressionRegressor())\n",
    "# manager.add_model(\"random_forest\", RandomForestRegressorWrapper())\n",
    "\n",
    "# Evaluate all models on training data with hypertuning and feature elimination\n",
    "time_start = time.time()\n",
    "results = manager.evaluate_all(\n",
    "    X_train, y_train,\n",
    "    hypertune=False,\n",
    "    hypertune_params={'n_trials': 2},\n",
    "    n_jobs=6,  # Set to -1 for all cores\n",
    "    cv_folds=5,\n",
    "    test_size=0.1,\n",
    "    feature_elimination=True,\n",
    "    fe_n_features=5,\n",
    "    fe_method='lasso',\n",
    "    return_plots=True  # set True if you want plots\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results (CV + Test):\")\n",
    "print(results)\n",
    "\n",
    "# Get best model by metric (e.g., 'rmse')\n",
    "best_model_name, best_model = manager.get_best_model(metric='rmse', higher_is_better=False)\n",
    "print(f\"\\nBest model by RMSE: {best_model_name}\")\n",
    "time_end = time.time()\n",
    "\n",
    "# Evaluate best model on the test set separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for evaluation: 1.0 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken for evaluation: {(time_end - time_start)//60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92923970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear_regression': {},\n",
       " 'random_forest': {},\n",
       " 'decision_tree': {},\n",
       " 'svr': {},\n",
       " 'knn_regression': {},\n",
       " 'gradient_boosting': {},\n",
       " 'elastic_net': {},\n",
       " 'bayesian_ridge': {},\n",
       " 'adaboost': {},\n",
       " 'lasso': {},\n",
       " 'ridge': {}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.get_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ccb775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b7343f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlinear_regression\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscatter\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'scatter'"
     ]
    }
   ],
   "source": [
    "manager.get_plots()['linear_regression']['scatter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b099f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv_scores': {'rmse': (np.float64(0.8040853634139525),\n",
       "   np.float64(0.1300242365136351)),\n",
       "  'mae': (np.float64(0.5472359711631587), np.float64(0.003277065357366334)),\n",
       "  'r2': (np.float64(0.5003432933141967), np.float64(0.1815338559505305))},\n",
       " 'test_scores': {'rmse': np.float64(0.717256211776781),\n",
       "  'mae': 0.5291309251606496,\n",
       "  'r2': 0.6206697989665664},\n",
       " 'plots': {'line': <Figure size 640x480 with 1 Axes>,\n",
       "  'scatter': <Figure size 640x480 with 1 Axes>}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76a28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Diabetes dataset...\n",
      "Dataset loaded successfully.\n",
      "Features shape: (442, 10)\n",
      "Target shape: (442,)\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Training set size: 353\n",
      "Test set size: 89\n",
      "\n",
      "Initializing the RegressorManager...\n",
      "RegressorManager initialized.\n",
      "Available models: ['linear_regression', 'random_forest', 'decision_tree', 'svr', 'knn_regression', 'gradient_boosting', 'elastic_net', 'bayesian_ridge', 'adaboost', 'lasso', 'ridge']\n",
      "\n",
      "Starting model evaluation...\n",
      "\n",
      "Evaluation completed in 21.32 seconds.\n",
      "\n",
      "--- Evaluation Results (Cross-Validation & Test Scores) ---\n",
      "                   cv_rmse_mean  cv_rmse_std  cv_mae_mean  cv_mae_std  \\\n",
      "model                                                                   \n",
      "linear_regression    159.822265     6.886711   148.749428    7.166158   \n",
      "knn_regression        60.000761     2.680674    48.495329    2.520772   \n",
      "svr                   61.197223     1.960027    48.288840    1.142992   \n",
      "elastic_net           56.585578     0.628126    45.462238    1.083399   \n",
      "decision_tree         63.212836     4.674362    49.976718    4.490057   \n",
      "bayesian_ridge        56.599163     0.761051    45.420951    1.039664   \n",
      "lasso                 56.607886     0.958284    45.413724    0.932507   \n",
      "ridge                 56.603404     0.961472    45.404618    0.928426   \n",
      "random_forest         58.000396     3.251003    46.806060    3.473705   \n",
      "gradient_boosting     66.761067     5.097685    51.941146    4.322863   \n",
      "adaboost              58.994146     2.211105    47.190764    2.550832   \n",
      "\n",
      "                   cv_r2_mean  cv_r2_std   test_rmse    test_mae   test_r2  \n",
      "model                                                                       \n",
      "linear_regression   -3.164418   0.460090  176.591760  168.075814 -4.800420  \n",
      "knn_regression       0.415328   0.037581   56.475035   47.610923  0.406758  \n",
      "svr                  0.389851   0.055681   62.315796   51.886763  0.277705  \n",
      "elastic_net          0.479335   0.028358   54.222396   44.936992  0.453140  \n",
      "decision_tree        0.350758   0.071953   59.941175   47.239719  0.331704  \n",
      "bayesian_ridge       0.478950   0.031150   54.396745   45.069592  0.449618  \n",
      "lasso                0.478588   0.035018   54.519214   45.173233  0.447137  \n",
      "ridge                0.478669   0.035053   54.522230   45.178689  0.447076  \n",
      "random_forest        0.453673   0.043718   57.348371   45.862191  0.388269  \n",
      "gradient_boosting    0.272175   0.107840   68.674440   51.281797  0.122779  \n",
      "adaboost             0.434636   0.033481   57.955675   46.057266  0.375244  \n",
      "\n",
      "--- Getting the Best Model ---\n",
      "Best performing model based on RMSE: 'elastic_net'\n",
      "\n",
      "Tuned Hyperparameters for elastic_net:\n",
      "{'alpha': 0.0002606045384968765, 'l1_ratio': 0.4125003207098882}\n",
      "\n",
      "--- Making Predictions with the Best Model ---\n",
      "Applying feature elimination to the training set...\n",
      "Training the final 'elastic_net' model on the full training set...\n",
      "Applying the same feature elimination to the test set...\n",
      "\n",
      "Predictions made on the processed test set.\n",
      "   Actual   Predicted\n",
      "0   219.0  142.367781\n",
      "1    70.0  179.226308\n",
      "2   202.0  141.546683\n",
      "3   230.0  289.992152\n",
      "4   111.0  123.762425\n",
      "5    84.0   96.944197\n",
      "6   242.0  253.484165\n",
      "7   272.0  188.410978\n",
      "8    94.0   86.970369\n",
      "9    96.0  113.561109\n",
      "\n",
      "Notebook execution finished.\n"
     ]
    }
   ],
   "source": [
    "from plantbrain_fastml.managers.regressor_manager import RegressorManager\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# It's good practice to wrap the entire script logic in a main function\n",
    "def main():\n",
    "    # --- 1. Load and Prepare the Dataset ---\n",
    "    print(\"Loading the Diabetes dataset...\")\n",
    "    data = load_diabetes()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "    print(\"\\nSplitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "    # --- 2. Initialize the Regressor Manager ---\n",
    "    print(\"\\nInitializing the RegressorManager...\")\n",
    "    manager = RegressorManager()\n",
    "    print(\"RegressorManager initialized.\")\n",
    "    print(\"Available models:\", list(manager.models.keys()))\n",
    "\n",
    "\n",
    "    # --- 3. Evaluate All Models Using Default Metrics ---\n",
    "    print(\"\\nStarting model evaluation...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = manager.evaluate_all(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        hypertune=True,\n",
    "        hypertune_params={'n_trials': 2},\n",
    "        hypertune_metrics='r2',\n",
    "        n_jobs=-2,\n",
    "        cv_folds=3,\n",
    "        test_size=0.2,\n",
    "        feature_elimination=True,\n",
    "        fe_n_features=5,\n",
    "        fe_method='lasso',\n",
    "        return_plots=True\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nEvaluation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    # --- 4. Display Results and Get Best Model ---\n",
    "    print(\"\\n--- Evaluation Results (Cross-Validation & Test Scores) ---\")\n",
    "    print(results)\n",
    "\n",
    "    # Check if results are not empty before proceeding\n",
    "    if not results.empty:\n",
    "        print(\"\\n--- Getting the Best Model ---\")\n",
    "        best_model_name, best_model_object = manager.get_best_model(metric='rmse', higher_is_better=False)\n",
    "        print(f\"Best performing model based on RMSE: '{best_model_name}'\")\n",
    "\n",
    "        hyperparams = manager.get_hyperparameters()\n",
    "        print(f\"\\nTuned Hyperparameters for {best_model_name}:\")\n",
    "        print(hyperparams.get(best_model_name))\n",
    "\n",
    "\n",
    "        # --- 5. Make Predictions with the Best Model ---\n",
    "        print(\"\\n--- Making Predictions with the Best Model ---\")\n",
    "\n",
    "        # The preprocessor was already fitted during the evaluate_all call.\n",
    "        # First, process the TRAINING data.\n",
    "        print(\"Applying feature elimination to the training set...\")\n",
    "        X_train_processed = best_model_object.preprocessor.transform(X_train)\n",
    "\n",
    "        # **** THIS IS THE NEW, CRUCIAL STEP ****\n",
    "        # Now, train the best model on the processed training data.\n",
    "        print(f\"Training the final '{best_model_name}' model on the full training set...\")\n",
    "        best_model_object.train(X_train_processed, y_train) # Use the .train() method of your wrapper\n",
    "\n",
    "        # Now, process the TEST data using the same preprocessor\n",
    "        print(\"Applying the same feature elimination to the test set...\")\n",
    "        X_test_processed = best_model_object.preprocessor.transform(X_test)\n",
    "\n",
    "        # Finally, make predictions on the processed test data\n",
    "        predictions = best_model_object.predict(X_test_processed)\n",
    "\n",
    "        print(\"\\nPredictions made on the processed test set.\")\n",
    "        sample_comparison = pd.DataFrame({'Actual': y_test.values, 'Predicted': predictions}).head(10)\n",
    "        print(sample_comparison)\n",
    "        plots=manager.get_plots()\n",
    "\n",
    "        if plots and best_model_name in plots:\n",
    "            scatter_plot = plots[best_model_name].get('scatter')\n",
    "            if scatter_plot:\n",
    "                print(\"Displaying Predicted vs. Actual scatter plot...\")\n",
    "                # In a script you would use scatter_plot.show(), in a notebook this will display it\n",
    "                display(scatter_plot)\n",
    "        else:\n",
    "            print(f\"No plots found for model '{best_model_name}'.\")\n",
    "    else:\n",
    "        print(\"\\nEvaluation produced no results. Cannot determine the best model.\")\n",
    "\n",
    "    print(\"\\nNotebook execution finished.\")\n",
    "\n",
    "# This is the crucial part!\n",
    "# This tells Python to only run the main() function when the script is executed directly.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b25413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "total_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f800c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Breast Cancer dataset...\n",
      "Dataset loaded successfully.\n",
      "Training set size: 455\n",
      "Test set size: 114\n",
      "\n",
      "RegressorManager initialized.\n",
      "Available models: ['random_forest', 'logistic_regression', 'svc']\n",
      "\n",
      "Starting model evaluation...\n",
      "\n",
      "Evaluation completed in 29.70 seconds.\n",
      "\n",
      "--- Evaluation Results ---\n",
      "                     cv_accuracy_mean  cv_accuracy_std  cv_precision_mean  \\\n",
      "model                                                                       \n",
      "svc                          0.945091         0.012126           0.940319   \n",
      "logistic_regression          0.967047         0.013941           0.973629   \n",
      "random_forest                0.958828         0.012168           0.956298   \n",
      "\n",
      "                     cv_precision_std  cv_recall_mean  cv_recall_std  \\\n",
      "model                                                                  \n",
      "svc                          0.020827        0.974534       0.020913   \n",
      "logistic_regression          0.020723        0.974172       0.016359   \n",
      "random_forest                0.024727        0.978460       0.018954   \n",
      "\n",
      "                     cv_f1_mean  cv_f1_std  cv_roc_auc_mean  cv_roc_auc_std  \\\n",
      "model                                                                         \n",
      "svc                    0.956775   0.009862         0.981338        0.011333   \n",
      "logistic_regression    0.973673   0.011389         0.993611        0.007789   \n",
      "random_forest          0.966880   0.011125         0.993369        0.005670   \n",
      "\n",
      "                     test_accuracy  test_precision  test_recall   test_f1  \\\n",
      "model                                                                       \n",
      "svc                       0.934066        0.963636     0.929825  0.946429   \n",
      "logistic_regression       0.934066        0.947368     0.947368  0.947368   \n",
      "random_forest             0.956044        0.964912     0.964912  0.964912   \n",
      "\n",
      "                     test_roc_auc  \n",
      "model                              \n",
      "svc                      0.970588  \n",
      "logistic_regression      0.980392  \n",
      "random_forest            0.975232  \n",
      "\n",
      "--- Getting the Best Model ---\n",
      "Best performing model based on Test ROC AUC: 'logistic_regression'\n",
      "\n",
      "--- Analysis of Best Model: logistic_regression ---\n",
      "\n",
      "Tuned Hyperparameters for logistic_regression:\n",
      "{'C': 15.504821294897665, 'solver': 'liblinear'}\n",
      "\n",
      "--- Making Predictions on the Test Set ---\n",
      "Applying feature elimination to the training set...\n",
      "Training the final 'logistic_regression' model on the full training set...\n",
      "Applying the same feature elimination to the test set...\n",
      "Sample Predictions:\n",
      "[0 1 0 1 0]\n",
      "\n",
      "Sample Probabilities (for class 0 and 1):\n",
      "[[1.00000000e+00 1.22168450e-14]\n",
      " [4.34448688e-05 9.99956555e-01]\n",
      " [9.76606977e-01 2.33930228e-02]\n",
      " [2.35505517e-01 7.64494483e-01]\n",
      " [1.00000000e+00 1.00283991e-14]]\n",
      "\n",
      "Notebook execution finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from plantbrain_fastml.managers.classifier_manager import ClassifierManager\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "print(\"Loading the Breast Cancer dataset...\")\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Split data, ensuring stratified split for classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# --- 2. Initialize the Classifier Manager ---\n",
    "manager = ClassifierManager()\n",
    "print(\"\\nRegressorManager initialized.\")\n",
    "print(\"Available models:\", list(manager.models.keys()))\n",
    "\n",
    "\n",
    "# --- 3. Define Metrics and Evaluate All Models ---\n",
    "\n",
    "# Define the metrics to calculate as a DICTIONARY\n",
    "# Note: roc_auc requires probability scores, which our BaseClassifier handles.\n",
    "classification_metrics_to_calculate = {\n",
    "    'accuracy': accuracy_score,\n",
    "    'precision': precision_score,\n",
    "    'recall': recall_score,\n",
    "    'f1': f1_score,\n",
    "    'roc_auc': roc_auc_score\n",
    "}\n",
    "\n",
    "print(\"\\nStarting model evaluation...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate all models, using 'roc_auc' as the goal for hyperparameter tuning\n",
    "results = manager.evaluate_all(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    metrics=classification_metrics_to_calculate,\n",
    "    hypertune=True,\n",
    "    hypertune_params={'n_trials': 10}, # n_trials can be increased for a more thorough search\n",
    "    hypertune_metrics='roc_auc',\n",
    "    n_jobs=3, # Use all available CPU cores\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nEvaluation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- 4. Display Results and Get Best Model ---\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(results)\n",
    "\n",
    "print(\"\\n--- Getting the Best Model ---\")\n",
    "# Get the best model based on the test set's roc_auc score\n",
    "best_model_name, best_model_object = manager.get_best_model(metric='roc_auc', higher_is_better=True)\n",
    "print(f\"Best performing model based on Test ROC AUC: '{best_model_name}'\")\n",
    "\n",
    "\n",
    "# --- 5. In-depth Analysis of the Best Model ---\n",
    "print(f\"\\n--- Analysis of Best Model: {best_model_name} ---\")\n",
    "\n",
    "# Get the detailed classification report from the fitted model\n",
    "# Note: This requires a custom method on the BaseClassifier, which you've added.\n",
    "report = best_model_object.get_classification_report()\n",
    "if report:\n",
    "    print(\"Classification Report on Test Set:\")\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "\n",
    "# Get tuned hyperparameters from the manager\n",
    "hyperparams = manager.get_hyperparameters()\n",
    "print(f\"\\nTuned Hyperparameters for {best_model_name}:\")\n",
    "print(hyperparams.get(best_model_name))\n",
    "\n",
    "\n",
    "# --- 6. Make Predictions on New Data ---\n",
    "print(\"\\n--- Making Predictions on the Test Set ---\")\n",
    "# Use the best model's own preprocessor to transform the test data\n",
    "X_test_processed = best_model_object.preprocessor.transform(X_test)\n",
    "\n",
    "# Get final class predictions\n",
    "print(\"Applying feature elimination to the training set...\")\n",
    "X_train_processed = best_model_object.preprocessor.transform(X_train)\n",
    "\n",
    "# **** THIS IS THE NEW, CRUCIAL STEP ****\n",
    "# Now, train the best model on the processed training data.\n",
    "print(f\"Training the final '{best_model_name}' model on the full training set...\")\n",
    "best_model_object.train(X_train_processed, y_train) # Use the .train() method of your wrapper\n",
    "\n",
    "# Now, process the TEST data using the same preprocessor\n",
    "print(\"Applying the same feature elimination to the test set...\")\n",
    "X_test_processed = best_model_object.preprocessor.transform(X_test)\n",
    "\n",
    "# Finally, make predictions on the processed test data\n",
    "predictions = best_model_object.predict(X_test_processed)\n",
    "\n",
    "# Get prediction probabilities\n",
    "probabilities = best_model_object.predict_proba(X_test_processed)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(predictions[:5])\n",
    "print(\"\\nSample Probabilities (for class 0 and 1):\")\n",
    "print(probabilities[:5])\n",
    "\n",
    "print(\"\\nNotebook execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c2172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
